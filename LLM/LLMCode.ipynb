{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f2e2d5-899c-435c-8710-909311c1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resources and credits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6a87d1-3a97-4360-b343-05edfd481a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3872544344.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install transformers\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#install\n",
    "pip install transformers\n",
    "pip install tensorflow\n",
    "# Install Flask and other required libraries\n",
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7dd994-aefc-4be3-be89-604f97ee920b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfplumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfplumber'"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import pdfplumber\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842ec018-e6d1-4b25-9f67-2fc9d6e4abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training\n",
    "\n",
    "\n",
    "def fetch_text_from_pdf(pdf_link):\n",
    "    try:\n",
    "        response = requests.get(pdf_link)\n",
    "        with open('temp.pdf', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        with pdfplumber.open('temp.pdf') as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or ''\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pdf_link}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c7b4d-bd81-48cf-84a9-021cefa5432c",
   "metadata": {},
   "source": [
    "Training adn testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5673cf6-3407-4782-ade8-2987d43c7724",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'chanakya', 'neeti', 'chapter'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Test preprocessing on a sample text\n",
    "sample_text = \"\"\"\n",
    "Chanakya Neeti, Chapter 1:\n",
    "A person should not be too honest.\n",
    "\"\"\"\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7405d4-d2e2-42d2-8432-a4d1eab040a7",
   "metadata": {},
   "source": [
    "Collecting and Preprocessing Data from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9109e348-ca7f-4417-ba82-8c17f2d3b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching <URL>: name 'requests' is not defined\n",
      "Error fetching <URL>: name 'requests' is not defined\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "books = [\n",
    "    {\"title\": \"Chanakya Neeti\", \"pdf_link\": \"<URL>\"},\n",
    "    {\"title\": \"Chanakya Neeti Darpan\", \"pdf_link\": \"<URL>\"},\n",
    "]\n",
    "\n",
    "for book in books:\n",
    "    pdf_link = book[\"pdf_link\"]\n",
    "    text = fetch_text_from_pdf(pdf_link)\n",
    "    if text:\n",
    "        processed_text = preprocess_text(text)\n",
    "        corpus.append(processed_text)\n",
    "\n",
    "for i, sample in enumerate(corpus[:3], 1):\n",
    "    print(f\"Sample {i}: {sample[:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f0a49-f18a-440d-8eed-41ea1962d362",
   "metadata": {},
   "source": [
    "Training the Custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e25dd8b-af41-4dbb-ac12-84f02d329500",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (265257713.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. Prepare Training Data\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411f6f01-3a22-4256-8ed4-4d0a575e53c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85071e82-506e-4de9-9666-a5d070138079",
   "metadata": {},
   "source": [
    "Define LLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114c84a7-72b5-4ac4-8e3f-839090343216",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (76599491.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    mport tensorflow as tf\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mport tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming corpus is your list of preprocessed texts\n",
    "X = [text.split() for text in corpus]  # Tokenized input texts\n",
    "y = [text[1:] for text in X]  # Target output (shifted input for training)\n",
    "\n",
    "# Convert words to integer indices\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(set([word for text in X for word in text]))}\n",
    "X = [[word_index[word] for word in text] for text in X]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set hyperparameters\n",
    "vocab_size = len(word_index) + 1  # Plus one for padding\n",
    "embedding_dim = 128\n",
    "max_seq_length = 50\n",
    "lstm_units = 256\n",
    "output_units = vocab_size\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length)\n",
    "X_val = pad_sequences(X_val, maxlen=max_seq_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c794c-a037-4a7b-a17d-d1c8bb1bfb51",
   "metadata": {},
   "source": [
    "Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66930bb-dcbb-4f5f-9e49-dda538df8997",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[1;32m      3\u001b[0m     Embedding(vocab_size, embedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_seq_length),\n\u001b[1;32m      4\u001b[0m     LSTM(lstm_units),\n\u001b[1;32m      5\u001b[0m     Dense(output_units, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_seq_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(output_units, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c3b08-1152-46e7-ba27-a25c38132c4f",
   "metadata": {},
   "source": [
    "#Train and Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895960a3-c095-45fc-865b-0998958d6866",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compile and train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('custom_llm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d32375-7c29-4c85-93ea-428657815f5e",
   "metadata": {},
   "source": [
    "Evaluation and Testing\n",
    "Evaluation Metrics\n",
    "\n",
    "    Perplexity: This metric measures how well the model’s predicted probability distribution matches the actual distribution of words. A lower perplexity indicates better performance.\n",
    "    BLEU Score: The BLEU (Bilingual Evaluation Understudy) score evaluates the quality of generated text by comparing it to reference texts. Higher scores suggest closer alignment to the reference output. Note that BLEU has limitations and may not capture the full nuance of human-generated responses.\n",
    "    \n",
    "Testing the Model\n",
    "\n",
    "    Generate Text Samples: Use the trained custom LLM to generate text based on a given input prompt.\n",
    "    Manual Review: Assess generated responses for accuracy, coherence, style, and alignment with Chanakya Neeti’s wisdom. While BLEU and perplexity provide quantitative insights, manual review is vital for capturing subtleties.\n",
    "    Iterate and Fine-Tune: Based on evaluation results, refine the model to enhance performance.\n",
    "    Additional Testing Strategies: Consider user studies or A/B testing to evaluate user satisfaction and the chatbot’s effectiveness in practical contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f274a3-15b7-4b66-86df-1df7367b5127",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFGPT2LMHeadModel, GPT2Tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(input_text, reference_text, max_length=50, num_return_sequences=5):\n",
    "    try:\n",
    "        # Load the fine-tuned LLM and tokenizer\n",
    "        model = TFGPT2LMHeadModel.from_pretrained(\"./fine_tuned_llm\")\n",
    "        tokenizer = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "        # Generate text samples\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "\n",
    "        # Decode generated text\n",
    "        generated_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "        logging.info(\"Generated Text: %s\", generated_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(reference_text, generated_text)]\n",
    "\n",
    "        # Calculate perplexity\n",
    "        input_ids = tokenizer.encode(reference_text[0], return_tensors=\"tf\")\n",
    "        logits = model(input_ids)[0]\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        perplexity = np.exp(cross_entropy(input_ids, logits).numpy())\n",
    "\n",
    "        return generated_text, bleu_scores, perplexity\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during evaluation: %s\", e)\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de690c0-5299-4569-b116-d9ace332e597",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b99fa44-4a50-4316-b0d0-b40526489481",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m reference_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe meaning of life is to be happy.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m generated_text, bleu_scores, perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m(input_text, reference_text)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "input_text = \"What is the meaning of life?\"\n",
    "reference_text = [\"The meaning of life is to be happy.\"]\n",
    "generated_text, bleu_scores, perplexity = evaluate_model(input_text, reference_text)\n",
    "\n",
    "if generated_text is not None:\n",
    "    print(\"Generated Text:\")\n",
    "    for text in generated_text:\n",
    "        print(text)\n",
    "    print(\"BLEU Scores:\", bleu_scores)\n",
    "    print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5077b35-5703-4786-948b-2cf66f75a80c",
   "metadata": {},
   "source": [
    "Fine-tuning and Optimization\n",
    "\n",
    "To fine-tune our custom Large Language Model (LLM), we load a pre-trained model like GPT-2 and unfreeze the last few layers for training. This process allows the model to adapt while retaining its foundational knowledge.\n",
    "Steps for Fine-tuning\n",
    "\n",
    "    Load the Pre-trained Model: Use the transformers library to load GPT-2 as the base model.\n",
    "    Unfreeze Layers: Unfreeze the last few layers to enable fine-tuning on the custom dataset.\n",
    "    Define the Optimizer: Choose the Adam optimizer with a learning rate of 1e−51e-51e−5. Note that this may need adjustment based on experimentation and dataset characteristics.\n",
    "    Compile the Model: Compile with sparse categorical cross-entropy as the loss function.\n",
    "    Train the Model: Train the model on the prepared datasets.\n",
    "    Save the Fine-tuned Model: Save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1125a177-fff2-4e40-87e5-7cbeba2fc541",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFGPT2LMHeadModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pre-trained GPT-2 model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Unfreeze specific layers for fine-tuning\n",
    "for layer in model.layers[-6:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Define the optimizer and learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Learning rate may require tuning\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Train the model on the custom dataset\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bf0aa-3691-416e-bd98-da7c81b7b25b",
   "metadata": {},
   "source": [
    "Deployment and User Interface\n",
    "\n",
    "This section will explore methods for deploying our fine-tuned LLM and creating a user interface to interact with it. We’ll utilize Next.js, TypeScript, and Google Material UI for the front end, while Python and Flask for the back end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207c7d9-f589-44ea-8b54-8c4cb44b37ea",
   "metadata": {},
   "source": [
    "Backend (Python and Flask) — app.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4cc4b49-6e02-4c08-9d2d-0b35f380b37c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFGPT2LMHeadModel, GPT2Tokenizer\n\u001b[1;32m      4\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "@app.route(\"/chatbot\", methods=[\"POST\"])\n",
    "def chatbot():\n",
    "    message = request.json[\"message\"]\n",
    "    input_ids = tokenizer.encode(message, return_tensors=\"tf\")\n",
    "    output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output_ids[0])\n",
    "\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a841d04-02bb-4a45-8eab-d7b8a17444a1",
   "metadata": {},
   "source": [
    "Frontend (Next.js, Material UI, and TypeScript) — pages/index.tsx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81e3f0-10b3-4e21-97f5-c1b8abc8c0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
